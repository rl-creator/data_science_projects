{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ryan Leeson, Keith Jennings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! scancel -u ryan.leeson -n sparkcluster "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sparkhpc.sparkjob:Submitted batch job 8835\n",
      "\n",
      "INFO:sparkhpc.sparkjob:Submitted cluster 2\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://b07:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>spark://b11:7077</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=spark://b11:7077 appName=pyspark-shell>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os, atexit, sys, findspark, sparkhpc, pyspark\n",
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql import SQLContext\n",
    "\n",
    "findspark.init()\n",
    "\n",
    "# specify your partition (unless you're OK with default)\n",
    "os.environ['SBATCH_PARTITION']='cpu24'\n",
    "\n",
    "sj = sparkhpc.sparkjob.sparkjob(\n",
    "    ncores = 10,                       # total number or cores\n",
    "    cores_per_executor = 5,            # parallelism of two executor\n",
    "    memory_per_core = 10240,           # memory per core in MB \n",
    "    walltime = \"4:0\"                   # hh:mm format\n",
    ")\n",
    "\n",
    "sj.wait_to_start()\n",
    "sc = sj.start_spark()\n",
    "scq = SQLContext(sc)\n",
    "\n",
    "def exitHandler(sj,sc):\n",
    "    try:\n",
    "        print('Trapped Exit cleaning up Spark Context')\n",
    "        sc.stop()\n",
    "    except:\n",
    "        pass\n",
    "    try:\n",
    "        print('Trapped Exit cleaning up Spark Job')\n",
    "        sj.stop()\n",
    "    except:\n",
    "        pass\n",
    "atexit.register(exitHandler,sj,sc);\n",
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "\n",
    "from pyspark.ml.feature import StopWordsRemover\n",
    "\n",
    "from pyspark.ml.feature import Tokenizer, RegexTokenizer\n",
    "from pyspark.sql.functions import col, udf, concat\n",
    "from pyspark.sql import functions as f\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "from pyspark.sql.functions import input_file_name\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "from pyspark.sql.functions import split, lower\n",
    "\n",
    "from pyspark.ml.feature import NGram, FeatureHasher, CountVectorizer, RegexTokenizer\n",
    "\n",
    "from pyspark.ml.linalg import Vectors, VectorUDT\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "\n",
    "from itertools import chain\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "from pyspark import keyword_only\n",
    "from pyspark.ml import Transformer\n",
    "from pyspark.ml.param.shared import HasInputCol, HasOutputCol, Param, Params, TypeConverters\n",
    "from pyspark.ml.util import DefaultParamsReadable, DefaultParamsWritable  \n",
    "from pyspark.sql.functions import udf, create_map, lit\n",
    "from pyspark.sql.types import ArrayType, StringType\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = scq.read.parquet ('Files/amazon_reviews.parquet').select ('overall', 'reviewText')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+\n",
      "|overall|          reviewText|\n",
      "+-------+--------------------+\n",
      "|    5.0|The King, the Mic...|\n",
      "|    5.0|  The kids loved it!|\n",
      "|    5.0|My students (3 & ...|\n",
      "+-------+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show (3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 75257650 entries in the dataset.\n"
     ]
    }
   ],
   "source": [
    "print (f'There are {df.count ()} entries in the dataset.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[overall: double, count: bigint, percent: double]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(df.groupBy ('overall')\n",
    " .count ()\n",
    " .withColumn ('percent', f.col ('count') / f.sum ('count').over (Window.partitionBy ()))\n",
    " .orderBy ('overall', ascending = False)\n",
    " #.show ()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After cleaning there are 62454271 entries remaining.\n"
     ]
    }
   ],
   "source": [
    "#   Remove entries with duplicates, oveall < 1, and NA values in 'reviewText'\n",
    "\n",
    "df_cleaned = df.where (df.overall > 0).dropDuplicates ().na.drop (subset = ['reviewText'])\n",
    "\n",
    "print (f'After cleaning there are {df_cleaned.count ()} entries remaining.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_counts = (df_cleaned.groupBy ('overall')\n",
    " .count ()\n",
    " .withColumn ('percent', f.col ('count') / f.sum ('count').over (Window.partitionBy ()))\n",
    " .orderBy ('overall', ascending = False)\n",
    ")\n",
    "\n",
    "#display (df_counts.toPandas ())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#   better way to find fractions\n",
    "\n",
    "df_frac = df_counts.toPandas ()\n",
    "\n",
    "df_frac['fraction'] = df_frac['count'].apply (lambda x: df_frac['count'].min () / x)\n",
    "\n",
    "fractions = df_frac[['overall', 'fraction']].set_index ('overall').T.to_dict ('record')[0]\n",
    "#fractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_equal = df_cleaned.sampleBy ('overall', fractions = fractions).cache ()\n",
    "\n",
    "(training, testing) = df_equal.randomSplit ([0.8, 0.2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#   Custom transformers\n",
    "\n",
    "\n",
    "class PolarityTransformer (Transformer, HasInputCol, HasOutputCol, DefaultParamsReadable, DefaultParamsWritable):\n",
    "    \n",
    "#   The polairtyTransformer uses the package 'TextBlob' to compute the pority of a string, how negative or how \n",
    "#   positive a string might be based on the words used. \n",
    "\n",
    "    @keyword_only\n",
    "    def __init__(self, inputCol=None, outputCol=None):\n",
    "        super (PolarityTransformer, self).__init__()\n",
    "        kwargs = self._input_kwargs\n",
    "        self.setParams(**kwargs)\n",
    "\n",
    "    @keyword_only\n",
    "    def setParams(self, inputCol=None, outputCol=None):\n",
    "        kwargs = self._input_kwargs\n",
    "        return self._set(**kwargs)\n",
    "\n",
    "    def _transform(self, dataset):\n",
    "        \n",
    "        def func (string):\n",
    "            return TextBlob (string).sentiment.polarity\n",
    "\n",
    "        t = StringType ()\n",
    "        out_col = self.getOutputCol ()\n",
    "        in_col = dataset[self.getInputCol ()]\n",
    "        return dataset.withColumn (out_col, udf (func, t)(in_col).cast ('double'))\n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "class WordSentimentTransformer (Transformer, HasInputCol, HasOutputCol, DefaultParamsReadable, DefaultParamsWritable):\n",
    "\n",
    "#   Similar to PolarityTransformer, WordSentimentTransformer uses 'TextBlob' to evaluate the positivity or the \n",
    "#   negativity of a word. WordSentimentTransformer examines each word in a string, determines its polarity (positive,\n",
    "#   negative, or netural), and returns the counts of positive, negative, and neutral words as a vector.\n",
    "    \n",
    "    @keyword_only\n",
    "    def __init__(self, inputCol=None, outputCol=None):\n",
    "        super (WordSentimentTransformer, self).__init__()\n",
    "        kwargs = self._input_kwargs\n",
    "        self.setParams(**kwargs)\n",
    "\n",
    "    @keyword_only\n",
    "    def setParams(self, inputCol=None, outputCol=None):\n",
    "        kwargs = self._input_kwargs\n",
    "        return self._set(**kwargs)\n",
    "\n",
    "    def _transform(self, dataset):\n",
    "        def WordsFreq (array):\n",
    "            \n",
    "            posWord = 0\n",
    "            negWord = 0\n",
    "            neutWord = 0\n",
    "\n",
    "            for word in array:\n",
    "                \n",
    "\n",
    "                if TextBlob (word).sentiment.polarity > 0:\n",
    "                    posWord += 1\n",
    "                elif TextBlob (word).sentiment.polarity < 0:\n",
    "                    negWord += 1\n",
    "                else:\n",
    "                    neutWord += 1\n",
    "\n",
    "            return posWord, negWord, neutWord\n",
    "\n",
    "        word_freq = udf (lambda a: Vectors.dense (WordsFreq (a)), VectorUDT ())\n",
    "\n",
    "        t = ArrayType (StringType ())\n",
    "        out_col = self.getOutputCol ()\n",
    "        in_col = dataset[self.getInputCol ()]\n",
    "        return dataset.withColumn (out_col, word_freq (in_col))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RandomForest classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RegexTokenizer (inputCol = \"reviewText\", outputCol = \"words\", pattern = \"\\\\W\", toLowercase = True)\n",
    "\n",
    "remover = StopWordsRemover (inputCol = tokenizer.getOutputCol (), outputCol = \"filtered\")\n",
    "\n",
    "wordsentiment = WordSentimentTransformer (inputCol = remover.getOutputCol (), outputCol = 'word_sentiment')\n",
    "\n",
    "polarity = PolarityTransformer (inputCol = 'reviewText', outputCol = 'polarity')\n",
    "\n",
    "\n",
    "assembler = VectorAssembler (inputCols = ['polarity', 'word_sentiment'], outputCol = 'features')\n",
    "\n",
    "\n",
    "#   model\n",
    "randforest = RandomForestClassifier (labelCol = 'overall', featuresCol = assembler.getOutputCol (), numTrees = 10, maxMemoryInMB = 1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline (stages = [polarity, tokenizer, remover, wordsentiment, assembler, randforest])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_model = pipeline.fit (training)\n",
    "rf_model.save ('models/rf_equalsize_model_amazon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import RandomForestClassificationModel\n",
    "from pyspark.ml import PipelineModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_model2 = PipelineModel.load ('models/rf_equalsize_model_amazon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "evaluatorAcc = MulticlassClassificationEvaluator (predictionCol = 'prediction', labelCol = 'overall', metricName = \"accuracy\")\n",
    "\n",
    "accuracy = evaluatorAcc.evaluate (rf_model2.transform (testing))\n",
    "print(\"Test Error of random forest classifier with balanced dataset: %g\" % (1.0 - accuracy))\n",
    "\n",
    "\n",
    "evaluatorf1 = MulticlassClassificationEvaluator (predictionCol = 'prediction', labelCol = 'overall', metricName = 'f1')\n",
    "fscore = evaluatorf1.evaluate (rf_model2.transform (testing))\n",
    "print (f'F1-score of random forest classifier with balanced dataset: {fscore}.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg = LogisticRegression (labelCol = 'overall', featuresCol = assembler.getOutputCol (), maxIter = 10, family = 'multinomial')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline2 = Pipeline (stages = [polarity, tokenizer, remover, wordsentiment, assembler, logreg])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_model = pipeline.fit (training)\n",
    "lr_model.save ('models/lr_equalsize_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = evaluatorAcc.evaluate (lr_model.transform (testing))\n",
    "print(\"Test Error of logistic regression with balanced dataset: %g\" % (1.0 - accuracy))\n",
    "\n",
    "fscore = evaluatorf1.evaluate (lr_model.transform (testing))\n",
    "print (f'F1-score of logistic regression with balanced dataset: {fscore}.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This code was run on the cpu24 and cpu32-mem systems. Unfortunately, neither run was able to be completed. The run on cpu24 was able to complete the fitting of the random forest classifier and evaluate the accuracy before the program was stopped. It appears the program was interupted because there was not enough memory to run the rest of the program. \n",
    "\n",
    "#### The random forest classification model had an accuracy of 65 %. The output is in the file 'amazon_model_fitting_spark_results2.txt'. The program was run with 20 cores on four nodes with 10240 MB of memory per core. The program ran for about seven hours before it was cut off."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-04-12 01:32:51 WARN  NativeCodeLoader:62 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\r\n",
      "2020-04-12 01:34:57 WARN  WindowExec:66 - No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\r\n",
      "There are 75257650 entries in the dataset.\r\n",
      "After cleaning there are 62454271 entries remaining.\r\n",
      "Test Error of random forest classifier with balanced dataset: 0.654913\r\n",
      "Trapped Exit cleaning up Spark Context\r\n",
      "Trapped Exit cleaning up Spark Job\r\n"
     ]
    }
   ],
   "source": [
    "#   Output from the run on cpu24\n",
    "\n",
    "! cat amazon_model_fitting_spark_results2.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Slurm job"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Script file to submit Slurm job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%file amazon_model_fitting_spark.script\n",
    "#!/bin/bash\n",
    "#SBATCH -J slurm-spark\n",
    "#SBATCH -t 1440 # runtime to request !!! in minutes !!!\n",
    "#SBATCH -o slurm-spark-%J.log # output extra o means overwrite\n",
    "#SBATCH -n 1 # requesting n tasks\n",
    "\n",
    "module load spark/jupyterhub\n",
    ". /global/software/jupyterhub-spark/anaconda3/etc/profile.d/conda.sh\n",
    "\n",
    "python amazon_model_fitting_spark.py > amazon_model_fitting_spark_results.txt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### .py file for fitting of balanced dataset\n",
    "\n",
    "This code was run on the TALC cpu32-bigmem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%file amazon_model_fitting_spark.py\n",
    "\n",
    "#   Spark \n",
    "\n",
    "import os, atexit, sys, findspark, sparkhpc, pyspark\n",
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql import SQLContext\n",
    "\n",
    "findspark.init()\n",
    "\n",
    "# specify your partition (unless you're OK with default)\n",
    "os.environ['SBATCH_PARTITION']='cpu32-bigmem'\n",
    "\n",
    "sj = sparkhpc.sparkjob.sparkjob(\n",
    "    ncores = 24,                          # total number or cores\n",
    "    cores_per_executor = 24,              # parallelism of a single executor\n",
    "    memory_per_core = 1100 * 1024 // 24,  # memory per core in MB\n",
    "    walltime = \"21:0\"                      # hh:mm format\n",
    ")\n",
    "\n",
    "sj.wait_to_start()\n",
    "sc = sj.start_spark()\n",
    "scq = SQLContext(sc)\n",
    "\n",
    "def exitHandler(sj,sc):\n",
    "    try:\n",
    "        print('Trapped Exit cleaning up Spark Context')\n",
    "        sc.stop()\n",
    "    except:\n",
    "        pass\n",
    "    try:\n",
    "        print('Trapped Exit cleaning up Spark Job')\n",
    "        sj.stop()\n",
    "    except:\n",
    "        pass\n",
    "atexit.register(exitHandler,sj,sc);\n",
    "\n",
    "\n",
    "###########################################################################################################################\n",
    "\n",
    "from textblob import TextBlob\n",
    "\n",
    "from pyspark.ml.feature import StopWordsRemover\n",
    "\n",
    "from pyspark.ml.feature import Tokenizer, RegexTokenizer\n",
    "from pyspark.sql.functions import col, udf, concat\n",
    "from pyspark.sql import functions as f\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "from pyspark.sql.functions import input_file_name\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "from pyspark.sql.functions import split, lower\n",
    "\n",
    "from pyspark.ml.feature import NGram, FeatureHasher, CountVectorizer, RegexTokenizer\n",
    "\n",
    "from pyspark.ml.linalg import Vectors, VectorUDT\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "from itertools import chain\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "from pyspark import keyword_only\n",
    "from pyspark.ml import Transformer\n",
    "from pyspark.ml.param.shared import HasInputCol, HasOutputCol, Param, Params, TypeConverters\n",
    "from pyspark.ml.util import DefaultParamsReadable, DefaultParamsWritable  \n",
    "from pyspark.sql.functions import udf, create_map, lit\n",
    "from pyspark.sql.types import ArrayType, StringType\n",
    "from pyspark.sql.import Window\n",
    "\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "import time\n",
    "\n",
    "###########################################################################################################################\n",
    "\n",
    "start = time.time ()\n",
    "\n",
    "\n",
    "#   Bring in amazon_reviews.parquet\n",
    "df = scq.read.parquet ('Files/amazon_reviews.parquet').select ('overall', 'reviewText')\n",
    "\n",
    "#   Count of entries in the original dataset\n",
    "print (f'There are {df.count ()} entries in the dataset.')\n",
    "\n",
    "\n",
    "#   Remove properties with duplicate IDs, remove entries where overall > 0, remove NAs\n",
    "\n",
    "df_cleaned = df.where (df.overall > 0).dropDuplicates ().na.drop (subset = ['reviewText'])\n",
    "\n",
    "#   Count of entries in the cleaned data\n",
    "print (f'After cleaning there are {df_cleaned.count ()} entries remaining.')\n",
    "\n",
    "\n",
    "#   Determining the fractions to create classes of equal size\n",
    "df_counts = (df_cleaned.groupBy ('overall')\n",
    " .count ()\n",
    " .withColumn ('percent', f.col ('count') / f.sum ('count').over (Window.partitionBy ()))\n",
    " .orderBy ('overall', ascending = False)\n",
    ")\n",
    "\n",
    "\n",
    "df_frac = df_counts.toPandas ()\n",
    "\n",
    "df_frac['fraction'] = df_frac['count'].apply (lambda x: df_frac['count'].min () / x)\n",
    "\n",
    "fractions = df_frac[['overall', 'fraction']].set_index ('overall').T.to_dict ('record')[0]\n",
    "\n",
    "#   Create a dataset with equal sized classes\n",
    "df_equal = df_cleaned.sampleBy ('overall', fractions = fractions).cache ()\n",
    "\n",
    "\n",
    "#   Getting training and testing splits\n",
    "(training, testing) = df_equal.randomSplit ([0.8, 0.2])\n",
    "\n",
    "\n",
    "\n",
    "##################################################################################################################\n",
    "#   Custom transformers\n",
    "\n",
    "class PolarityTransformer (Transformer, HasInputCol, HasOutputCol, DefaultParamsReadable, DefaultParamsWritable):\n",
    "    \n",
    "    @keyword_only\n",
    "    def __init__(self, inputCol=None, outputCol=None):\n",
    "        super (PolarityTransformer, self).__init__()\n",
    "        kwargs = self._input_kwargs\n",
    "        self.setParams(**kwargs)\n",
    "\n",
    "    @keyword_only\n",
    "    def setParams(self, inputCol=None, outputCol=None):\n",
    "        kwargs = self._input_kwargs\n",
    "        return self._set(**kwargs)\n",
    "\n",
    "    def _transform(self, dataset):\n",
    "        \n",
    "        def func (string):\n",
    "            return TextBlob (string).sentiment.polarity\n",
    "\n",
    "        t = StringType ()\n",
    "        out_col = self.getOutputCol ()\n",
    "        in_col = dataset[self.getInputCol ()]\n",
    "        return dataset.withColumn (out_col, udf (func, t)(in_col).cast ('double'))\n",
    "\n",
    "\n",
    "class WordSentimentTransformer (Transformer, HasInputCol, HasOutputCol, DefaultParamsReadable, DefaultParamsWritable):\n",
    "    \n",
    "    @keyword_only\n",
    "    def __init__(self, inputCol=None, outputCol=None):\n",
    "        super (WordSentimentTransformer, self).__init__()\n",
    "        kwargs = self._input_kwargs\n",
    "        self.setParams(**kwargs)\n",
    "\n",
    "    @keyword_only\n",
    "    def setParams(self, inputCol=None, outputCol=None):\n",
    "        kwargs = self._input_kwargs\n",
    "        return self._set(**kwargs)\n",
    "\n",
    "    def _transform(self, dataset):\n",
    "        def WordsFreq (array):\n",
    "            \n",
    "            posWord = 0\n",
    "            negWord = 0\n",
    "            neutWord = 0\n",
    "\n",
    "            for word in array:\n",
    "                \n",
    "\n",
    "                if TextBlob (word).sentiment.polarity > 0:\n",
    "                    posWord += 1\n",
    "                elif TextBlob (word).sentiment.polarity < 0:\n",
    "                    negWord += 1\n",
    "                else:\n",
    "                    neutWord += 1\n",
    "\n",
    "            return posWord, negWord, neutWord\n",
    "\n",
    "        word_freq = udf (lambda a: Vectors.dense (WordsFreq (a)), VectorUDT ())\n",
    "\n",
    "        t = ArrayType (StringType ())\n",
    "        out_col = self.getOutputCol ()\n",
    "        in_col = dataset[self.getInputCol ()]\n",
    "        return dataset.withColumn (out_col, word_freq (in_col))\n",
    "\n",
    "##################################################################################################################\n",
    "\n",
    "\n",
    "#   Set transformers \n",
    "\n",
    "tokenizer = RegexTokenizer (inputCol = \"reviewText\", outputCol = \"words\", pattern = \"\\\\W\", toLowercase = True)\n",
    "\n",
    "remover = StopWordsRemover (inputCol = tokenizer.getOutputCol (), outputCol = \"filtered\")\n",
    "\n",
    "wordsentiment = WordSentimentTransformer (inputCol = remover.getOutputCol (), outputCol = 'word_sentiment')\n",
    "\n",
    "polarity = PolarityTransformer (inputCol = 'reviewText', outputCol = 'polarity')\n",
    "\n",
    "\n",
    "assembler = VectorAssembler (inputCols = ['polarity', 'word_sentiment'], outputCol = 'features')\n",
    "\n",
    "\n",
    "#   Set Random Forest model\n",
    "randforest = RandomForestClassifier (labelCol = 'overall', featuresCol = assembler.getOutputCol (), numTrees = 10, maxMemoryInMB = 1024)\n",
    "\n",
    "####################################################################################################################\n",
    "\n",
    "#   Random Forest model\n",
    "\n",
    "pipeline = Pipeline (stages = [polarity, tokenizer, remover, wordsentiment, assembler, randforest])\n",
    "\n",
    "#   Fitting model\n",
    "rf_model = pipeline.fit (training)\n",
    "\n",
    "#   Saving model\n",
    "rf_model.save ('models/rf_equalsize_model_amazon')\n",
    "\n",
    "#########\n",
    "\n",
    "#   Evaluating model\n",
    "\n",
    "evaluatorAcc = MulticlassClassificationEvaluator (predictionCol = 'prediction', labelCol = 'overall', metricName = \"accuracy\")\n",
    "\n",
    "accuracy = evaluatorAcc.evaluate (rf_model.transform (testing))\n",
    "print(\"Test Error of random forest classifier with balanced dataset: %g\" % (1.0 - accuracy))\n",
    "\n",
    "\n",
    "evaluatorf1 = MulticlassClassificationEvaluator (predictionCol = 'prediction', labelCol = 'overall', metricName = 'f1')\n",
    "\n",
    "fscore = evaluatorf1.evaluate (rf_model.transform (testing))\n",
    "print (f'F1-score of random forest classifier with balanced dataset: {fscore}.')\n",
    "\n",
    "####################################################################################################################\n",
    "\n",
    "#   Logistic Regression model\n",
    "\n",
    "logreg = LogisticRegression (labelCol = 'overall', featuresCol = assembler.getOutputCol (), maxIter = 10, family = 'multinomial')\n",
    "\n",
    "pipeline2 = Pipeline (stages = [polarity, tokenizer, remover, wordsentiment, assembler, logreg])\n",
    "\n",
    "#   Fitting model\n",
    "lr_model = pipeline.fit (training)\n",
    "\n",
    "#   Saving model\n",
    "lr_model.save ('models/lr_equalsize_model_amazon')\n",
    "\n",
    "##########\n",
    "\n",
    "#   Evaluating model\n",
    "\n",
    "accuracy = evaluatorAcc.evaluate (lr_model.transform (testing))\n",
    "print(\"Test Error of logistic regression with balanced dataset: %g\" % (1.0 - accuracy))\n",
    "\n",
    "fscore = evaluatorf1.evaluate (lr_model.transform (testing))\n",
    "print (f'F1-score of logistic regression with balanced dataset: {fscore}.')\n",
    "\n",
    "print (f'Program took {time.time () - start} seconds.')\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

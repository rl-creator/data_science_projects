{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/kwjennin/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from  pyspark.sql.functions import input_file_name\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import split, col, lower\n",
    "from pyspark.ml.feature import StopWordsRemover\n",
    "from pyspark.ml.feature import Tokenizer, RegexTokenizer\n",
    "from pyspark.sql.functions import col, udf, concat \n",
    "from pyspark.ml import Pipeline\n",
    "from  pyspark.sql.functions import input_file_name\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import split, lower\n",
    "from pyspark.ml.feature import NGram, FeatureHasher, CountVectorizer, RegexTokenizer\n",
    "import re\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from textblob import TextBlob\n",
    "from urllib.request import urlopen\n",
    "import urllib\n",
    "import requests\n",
    "import io\n",
    "import multiprocessing, os\n",
    "from multiprocessing.pool import ThreadPool\n",
    "import pyarrow\n",
    "import numpy as np\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from pyspark.sql.types import StringType, FloatType\n",
    "from pyspark.sql.functions import udf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)\n",
      "              8861     cpu24 jupyterh kwjennin  R    2:23:47      1 b09\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "scancel -u kwjennin -n sparkcluster\n",
    "squeue -u kwjennin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sparkhpc.sparkjob:Submitted batch job 8892\n",
      "\n",
      "INFO:sparkhpc.sparkjob:Submitted cluster 0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://b09:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>spark://b07:7077</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=spark://b07:7077 appName=pyspark-shell>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os, atexit, sys, findspark, sparkhpc, pyspark\n",
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql import SQLContext\n",
    "\n",
    "findspark.init()\n",
    "\n",
    "os.environ['SBATCH_PARTITION']='cpu24'\n",
    "\n",
    "sj = sparkhpc.sparkjob.sparkjob(\n",
    "    ncores = 10,                          # total number or cores\n",
    "    cores_per_executor = 5,              # parallelism of a single executor\n",
    "    memory_per_core = 10240,  # memory per core in MB\n",
    "    walltime = \"3:0\"                      # hh:mm format\n",
    ")\n",
    "\n",
    "sj.wait_to_start()\n",
    "sc = sj.start_spark()\n",
    "scq = SQLContext(sc)\n",
    "\n",
    "def exitHandler(sj,sc):\n",
    "    try:\n",
    "        print('Trapped Exit cleaning up Spark Context')\n",
    "        sc.stop()\n",
    "    except:\n",
    "        pass\n",
    "    try:\n",
    "        print('Trapped Exit cleaning up Spark Job')\n",
    "        sj.stop()\n",
    "    except:\n",
    "        pass\n",
    "atexit.register(exitHandler,sj,sc);\n",
    "sc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text analysis of Amazon product reviews\n",
    "\n",
    "### DS 608 Project\n",
    "##### By Keith Jenning, Ryan Leeson"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "#### Sentiment analysis is a common problem in the field of machine learning. In this project, we attempt to build a classifier to predict the Star rating of an Amazon review using the text of the review for individual products. The advantage of using the Amazon review dataset is that it does not only include text-based data, it also includes an evaluation system in the form of a five star rating, where one and two stars show dislike or displeasure, four and five show people like and enjoy something, and three stars is a neutral position. With review texts and the associated star rating a model can be trained to evaluate a post and classifying it with a star rating to indicate whether the post indicates like or dislike regard the subject of the post. To do this, we make use of the TextBlob package to determine how positive or negative a review is, as well as determine the number of positive or negative words in the review, and relating it the Star rating of the product.\n",
    "\n",
    "\n",
    "#### Note: Due to issues running the file for over 18 hours on the full dataset, we were unable to have our model complete. Thus, we have chosen a single file in order to demonstrate its performance on a sample of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "#### The datset was was compiled by Jianomo Ni, a Ph.D. student from the University of California San Diego. The dataset is hosted on a github repository controlled by Mr. Ni. \n",
    "\n",
    "#### The dataset is stored as 29 seperate Json files. Each file consists of the reviews for each product category. The files vary in size from ~100 MB to the largest file, which are book reviews which is ~7 GB. Overall, all the files combined contain approximately 14 GB of data.\n",
    "\n",
    "#### The data include product reviews taken from 1996 up to the year 2018. The features of the dataset include response variable, 'overall,' which is overall rating of the product on a five point scale. Other features include the 'asin' which is a product identifier, 'image' which is a link to an image of the product, if available. For the reviews, there is the 'reviewText' which is the full review text written by the review author. Other features included are the time the review was made, as a date and as a UNIX timestamp, 'revieTime' and 'unixReviewTime.' There are also the reviewer's name, 'reviewerName,' and ID; 'reviewerID,' the 'summary,' a short text briefly stating the authors thoughts, 'verified' varifying the author of the product made the purchase, and 'vote' summarising whether the review received up or down votes.\n",
    "\n",
    "#### The dataset was pre-cleaned by Mr. Ni. Mr. Ni has versions of the files available through his github repo. The master file, containing all reviews collected by Mr. Ni, includes over 233 million reviews and is approximately 34 GB in size. The dataset used is referred to as a 5-core dataset. The title is defined to indicate that only products with a minimum of five reivews or reviews who have left a minimum of five reviews were included."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methods\n",
    "\n",
    "### File collection\n",
    "#### The code used to download the files from Mr. Ni's gihub page is provided in a different file titled 'batch_program_for_download.ipynb.'\n",
    "\n",
    "\n",
    "#### The download was done through a programmed written as a SLURM job run on the University of Calgary's TALC cluster system. Using one node and 16 cores, the job took 50.3 minutes (3020.565 s) to download all the files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting files to parquet\n",
    "\n",
    "#### Due to issues running the full 14GB file in Jupyter and the Google cluster, we selected a single file to run the model on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_reviews = scq.read.json('/home/kwjennin/Data608/Project/Files/*.json.gz')\n",
    "df_reviews = scq.read.json('/home/kwjennin/Data608/Project/Files/appliances.json.gz')\n",
    "#   There is a 'Files' folder where all the files were stored.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "(df_reviews\n",
    " ##########\n",
    " #   Adding a category feature.\n",
    " .withColumn('filename', input_file_name())\n",
    " .withColumn('file_cn', split(col('filename'), 'Files/').getItem(1))\n",
    " #   This line is dependent on how files were organised\n",
    " .withColumn('category', lower(split(col('file_cn'), '_5.json').getItem(0)))\n",
    " ##########\n",
    " .select(['asin', 'overall', 'reviewText', 'reviewTime', 'reviewerID', 'summary', 'unixReviewTime', 'verified', 'category'])\n",
    " .write.parquet('/home/kwjennin/Data608/Project/Files/amazon_reviews.parquet', mode = 'overwrite', partitionBy = 'category')\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The original downloaded files were stored in json format with Gzip compression. Using pyspark the data was read into a spark dataframe and exported as new files in parquet format using snappy compression.\n",
    "\n",
    "#### Using the titles of the files, a new category column was added to the data. During the creation of the parquet files, the category was used to partion the parquets, resulting in files like those originally downloaded.\n",
    "\n",
    "#### A custom schema was not implemented for reading the json files. Two features, 'photo' and 'style,' contained nested structures. These nested structures caused issues when reading the json files. Also, the nested comp\n",
    "\n",
    "#### During this process, two columns were removed from the dataset, 'photo' and 'style.' \n",
    "\n",
    "#### Two columns in the dataset, 'photo' and 'style', were nested. These nested structures were different for each file, making it difficult to create a custom schema for the files. Testing several files showed that read.json () was able to infer the schema of the dataset without issue. As a result, all the files could be imported and merged with one read.json () command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+\n",
      "|overall|          reviewText|\n",
      "+-------+--------------------+\n",
      "|    5.0|I like this as a ...|\n",
      "|    5.0|           good item|\n",
      "|    5.0|Fit my new LG dry...|\n",
      "+-------+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#   importing dataset, only columns 'overall' and 'reviewText\n",
    "\n",
    "reviews_labels = scq.read.parquet('/home/kwjennin/Data608/Project/Files/amazon_reviews.parquet').select ('overall', 'reviewText')\n",
    "\n",
    "reviews_labels.show(3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sample_df = reviews_labels.sample(False, 0.2)\n",
    "\n",
    "pdf = sample_df.toPandas()\n",
    "\n",
    "data = pdf['overall']\n",
    "data = data.astype('int64')\n",
    "counts = np.bincount(data)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.bar(range(6), counts, width=1, align='center', edgecolor = 'black')\n",
    "ax.set(xticks=range(6), xlim=[0, 6])\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifier\n",
    "\n",
    "#### We load the necessary elements to perform sentiment analysis on the review text column of the dataframe. Following that we create columns using user-defined functions. The main output columns from the functions that will be used for the classifier are the sentence polarity, positive word count, and negative word count.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+------------+------------+\n",
      "|Overall|  sentencePolarity|posWordCount|negWordCount|\n",
      "+-------+------------------+------------+------------+\n",
      "|    5.0|               0.4|           1|           0|\n",
      "|    5.0|               0.7|           1|           0|\n",
      "|    5.0|0.5121212121212121|           3|           0|\n",
      "+-------+------------------+------------+------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#create new columns\n",
    "def full(string):\n",
    "    stemmer = PorterStemmer()\n",
    "\n",
    "    stop = stopwords.words('english')\n",
    "\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    \n",
    "    ls = []\n",
    "    stop = stopwords.words('english')\n",
    "    if string is not None:\n",
    "        for word in string.split():\n",
    "            if word not in stop:\n",
    "                ls.append(word)\n",
    "    text = ' '.join(ls)\n",
    "    sentencePolarity = TextBlob(string).sentiment.polarity\n",
    "    \n",
    "    tokenized = tokenizer.tokenize(text)\n",
    "    \n",
    "    polarityPos = []\n",
    "    for word in tokenized:\n",
    "        if TextBlob(word).sentiment.polarity > 0:\n",
    "            polarityPos.append(word)\n",
    "            \n",
    "    polarityNeg = []\n",
    "    for word in tokenized:\n",
    "        if TextBlob(word).sentiment.polarity < 0:\n",
    "            polarityNeg.append(word)\n",
    "            \n",
    "    posWordCount = sum([polarityPos.count(w) for w in polarityPos])\n",
    "    negWordCount = sum([polarityNeg.count(w) for w in polarityNeg])\n",
    "    \n",
    "    return sentencePolarity, posWordCount, negWordCount\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"sentencePolarity\", DoubleType(), False),\n",
    "    StructField(\"posWordCount\", IntegerType(), False),\n",
    "    StructField(\"negWordCount\", IntegerType(), False)])\n",
    "\n",
    "full_udf = udf(full, schema)\n",
    "\n",
    "reviews_labels = reviews_labels.withColumn(\"Output\", full_udf(reviews_labels.reviewText))\n",
    "\n",
    "reviews_labels = reviews_labels.select(\"Overall\", \"Output.*\")\n",
    "\n",
    "reviews_labels.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We select the columns that are required for the classifier, convert the columns to the necessary type, and then give a count of each class in the dataframe. Following that, we trained a random forest classifier to predict the class label. The accuracy was approximately 70% which corresponds to the percent of 5 Star reviews in the dataframe. Further investigation showed that the majority of the labels predicted were 5 Star. Due to this labelling issue, we will attempt another classification method: the one vs. rest classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+\n",
      "|overall|   n|\n",
      "+-------+----+\n",
      "|    1.0|   9|\n",
      "|    4.0| 222|\n",
      "|    3.0| 421|\n",
      "|    2.0|  13|\n",
      "|    5.0|1612|\n",
      "+-------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "reviews_labels_select = reviews_labels.select('overall', 'sentencePolarity', 'posWordCount', 'negWordCount')\n",
    "\n",
    "for c in reviews_labels_select.columns:\n",
    "    reviews_labels_select = reviews_labels_select.withColumn(c, reviews_labels_select[c].cast('double'))\n",
    "\n",
    "import pyspark.sql.functions as f\n",
    "\n",
    "reviews_labels_select.groupBy('overall').count().select('overall', f.col('count').alias('n')).show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Our attempt to implement the one vs. rest classifier ran into the same issue as the random forest classifier: the majority of predictions were 5 Star. Unfortunately, there is no ability in PySpark to weight columns due to an imbalanced dataset for the random forest or the one vs. rest algorithms. It is, however, implemented in the logistic regression. Thus, our next attempted algorithm will be the multinomial logistic regression.\n",
    "\n",
    "#### As can be seen in the output, the weighting has reduced our accuracy. Although this is a decrease in our performance metric, we can now be confident that the classifier is performing a more intelligent prediction instead of an almost constant prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1.0: 50.6, 4.0: 2.0513513513513515, 3.0: 1.081710213776722, 2.0: 35.03076923076923, 5.0: 0.2825062034739454}\n"
     ]
    }
   ],
   "source": [
    "#Begin Multinomial Logistic Regression with Weights\n",
    "df = reviews_labels_select.withColumnRenamed('overall', 'label')\n",
    "\n",
    "df = df.select('label', 'sentencePolarity', 'posWordCount', 'negWordCount')\n",
    "\n",
    "#create column weights\n",
    "label_freq = df.select(\"label\").groupBy(\"label\").count().collect()\n",
    "unique_label = [x[\"label\"] for x in label_freq]\n",
    "total_label = sum([x[\"count\"] for x in label_freq])\n",
    "unique_label_count = len(label_freq)\n",
    "bin_count = [x[\"count\"] for x in label_freq]\n",
    "\n",
    "label_weights = {i: ii for i, ii in zip(unique_label, total_label / (unique_label_count * np.array(bin_count)))}\n",
    "print(label_weights)\n",
    "\n",
    "from itertools import chain\n",
    "\n",
    "mapping_expr = f.create_map([f.lit(x) for x in chain(*label_weights.items())])\n",
    "\n",
    "df = df.withColumn(\"weight\", mapping_expr.getItem(f.col(\"label\")))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#assemble feature vector\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "features = ['sentencePolarity', 'posWordCount', 'negWordCount']\n",
    "assembled = VectorAssembler(inputCols=features, outputCol=\"features\")\n",
    "\n",
    "df = assembled.transform(df.select(['label', 'sentencePolarity', 'posWordCount', 'negWordCount', 'weight'])).cache()\n",
    "\n",
    "df = df.select('label', 'features', 'weight')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split data and initialize model\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "(training, test) = df.randomSplit([0.8, 0.2])\n",
    "\n",
    "lr = LogisticRegression(weightCol = 'weight')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fit the model on training set\n",
    "lrModel = lr.fit(training)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+--------------------------------+\n",
      "|prediction|label|features                        |\n",
      "+----------+-----+--------------------------------+\n",
      "|1.0       |1.0  |[0.04393939393939394,2.0,1.0]   |\n",
      "|5.0       |1.0  |[0.07562858017403473,18.0,8.0]  |\n",
      "|3.0       |2.0  |[-0.019301470588235305,19.0,3.0]|\n",
      "+----------+-----+--------------------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#get predictions on test set\n",
    "predictions = lrModel.transform(test)\n",
    "\n",
    "predictions.select(\"prediction\", \"label\", \"features\").show(3, truncate = False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F Score = 0.521834\n"
     ]
    }
   ],
   "source": [
    "#retrieve evaluation metrics\n",
    "from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "\n",
    "results = predictions.select('prediction', 'label')\n",
    "predictionAndLabels=results.rdd\n",
    "metrics = MulticlassMetrics(predictionAndLabels)\n",
    "f1Score = metrics.fMeasure()\n",
    "\n",
    "print(\"F Score = %g\" % (f1Score))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We next compare the weighted multinomial logistic regression with one that samples an equal number of classes without the weight column. We can see the accuracy decreases. Additionally, this data sample is no longer 'big data'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Begin Multinomial Logistic Regression with Equal Samples\n",
    "df = reviews_labels_select.withColumnRenamed('overall', 'label')\n",
    "\n",
    "df = df.select('label', 'sentencePolarity', 'posWordCount', 'negWordCount')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#assemble feature vector\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "features = ['sentencePolarity', 'posWordCount', 'negWordCount']\n",
    "assembled = VectorAssembler(inputCols=features, outputCol=\"features\")\n",
    "\n",
    "df = assembled.transform(df.select(['label', 'sentencePolarity', 'posWordCount', 'negWordCount'])).cache()\n",
    "\n",
    "df = df.select('label', 'features')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sample each class in equal sizes, split data, and initialize model\n",
    "sampled = df.sampleBy('label', fractions = {1:0.4793, 2:1, 3:0.4499, 4:0.2384, 5:0.1204})\n",
    "\n",
    "(training, test) = sampled.randomSplit([0.8, 0.2])\n",
    "\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "lr = LogisticRegression()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fit model on training data\n",
    "lrModel = lr.fit(training)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+---------+\n",
      "|prediction|label|features |\n",
      "+----------+-----+---------+\n",
      "|1.0       |1.0  |(3,[],[])|\n",
      "|1.0       |1.0  |(3,[],[])|\n",
      "|1.0       |1.0  |(3,[],[])|\n",
      "+----------+-----+---------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#compare model with test set\n",
    "predictions = lrModel.transform(test)\n",
    "\n",
    "predictions.select(\"prediction\", \"label\", \"features\").show(3, truncate = False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 0.317143\n",
      "F Score = 0.317143\n"
     ]
    }
   ],
   "source": [
    "#get performance metrics\n",
    "from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "\n",
    "results = predictions.select('prediction', 'label')\n",
    "predictionAndLabels=results.rdd\n",
    "metrics = MulticlassMetrics(predictionAndLabels)\n",
    "f1Score = metrics.fMeasure()\n",
    "\n",
    "print(\"F Score = %g\" % (f1Score))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Next we tune our hyperparameters using cross validation for multinomial logistic regression. The accuracy only marginally improves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cross Validation Logistic Regression\n",
    "df = reviews_labels_select.withColumnRenamed('overall', 'label')\n",
    "\n",
    "df = df.select('label', 'sentencePolarity', 'posWordCount', 'negWordCount')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1.0: 50.6, 4.0: 2.0513513513513515, 3.0: 1.081710213776722, 2.0: 35.03076923076923, 5.0: 0.2825062034739454}\n"
     ]
    }
   ],
   "source": [
    "#get class weights\n",
    "label_freq = df.select(\"label\").groupBy(\"label\").count().collect()\n",
    "unique_label = [x[\"label\"] for x in label_freq]\n",
    "total_label = sum([x[\"count\"] for x in label_freq])\n",
    "unique_label_count = len(label_freq)\n",
    "bin_count = [x[\"count\"] for x in label_freq]\n",
    "\n",
    "label_weights = {i: ii for i, ii in zip(unique_label, total_label / (unique_label_count * np.array(bin_count)))}\n",
    "print(label_weights)\n",
    "\n",
    "import pyspark.sql.functions as f\n",
    "from itertools import chain\n",
    "\n",
    "mapping_expr = f.create_map([f.lit(x) for x in chain(*label_weights.items())])\n",
    "\n",
    "df = df.withColumn(\"weight\", mapping_expr.getItem(f.col(\"label\")))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#assemble feature vector\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "features = ['sentencePolarity', 'posWordCount', 'negWordCount']\n",
    "assembled = VectorAssembler(inputCols=features, outputCol=\"features\")\n",
    "\n",
    "df = assembled.transform(df.select(['label', 'sentencePolarity', 'posWordCount', 'negWordCount', 'weight'])).cache()\n",
    "\n",
    "df = df.select('label', 'features', 'weight')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split data and initialize model\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.tuning import TrainValidationSplit, ParamGridBuilder, CrossValidator\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "(training, test) = df.randomSplit([0.8, 0.2])\n",
    "\n",
    "lr = LogisticRegression(weightCol = 'weight')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cross validate\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator(metricName=\"accuracy\")\n",
    "\n",
    "grid = ParamGridBuilder().addGrid(lr.maxIter, [10, 20, 25]) \\\n",
    "                                .addGrid(lr.regParam, [0, 0.01, 0.05, 0.1, 0.5, 1]) \\\n",
    "                                .addGrid(lr.elasticNetParam, [0.0, 0.1, 0.5, 0.8, 1]) \\\n",
    "                                .build()\n",
    "\n",
    "lr_cv = CrossValidator(estimator=lr, estimatorParamMaps=grid, \\\n",
    "                        evaluator=evaluator, numFolds=3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fit model on training data\n",
    "lrModel = lr_cv.fit(training)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+--------------------------------+\n",
      "|prediction|label|features                        |\n",
      "+----------+-----+--------------------------------+\n",
      "|1.0       |2.0  |[-0.019301470588235305,19.0,3.0]|\n",
      "|1.0       |2.0  |[0.014772727272727274,2.0,1.0]  |\n",
      "|2.0       |2.0  |[0.4,1.0,0.0]                   |\n",
      "+----------+-----+--------------------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#choose best model and compare model with test set\n",
    "bestModel = lrModel.bestModel\n",
    "\n",
    "predictions = bestModel.transform(test)\n",
    "\n",
    "predictions.select(\"prediction\", \"label\", \"features\").show(3, truncate = False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F Score = 0.477895\n"
     ]
    }
   ],
   "source": [
    "#get performance metrics\n",
    "from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "\n",
    "results = predictions.select('prediction', 'label')\n",
    "predictionAndLabels=results.rdd\n",
    "metrics = MulticlassMetrics(predictionAndLabels)\n",
    "f1Score = metrics.fMeasure()\n",
    "\n",
    "print(\"F Score = %g\" % (f1Score))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "#### This project was intended to show a method of classifying Amazon reviews into their appropriate 5 Star ratings. The data posed many problems both with the challenges of the size of the data and with fitting an accurate model. Although the final model's accuracy is better than a random algorithm which would yield 20% accuracy, we note that it underperforms a constant choice algorithm which yields 70% accuracy. This leads to the question of whether it is better to predict based off of the most common class in order to achieve a higher accuracy, or to use an algorithm which attempts to use features to predict a class but ultimately ahieves a lower accuracy. Perhaps better feature engineering would have yielded better results or perhaps it is the case that ratings are not always predictable based off of review text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Future work\n",
    "\n",
    "In future more can be done to attempt to improve the predictions of the model. This includes \n",
    "* Determine the issue with the large datset\n",
    "* Selection another model, possibly a random forest classifier\n",
    "* Over-estimating the smaller sized classes\n",
    "* Using One-vs-rest classification\n",
    "* Examine the influence of category\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "#### 1. Amazon Review Data (2018): https://nijianmo.github.io/amazon/index.html\n",
    "#### 2. https://danvatterott.com/blog/2019/11/18/balancing-model-weights-in-pyspark/\n",
    "#### 3. https://towardsdatascience.com/multi-class-metrics-made-simple-part-ii-the-f1-score-ebe8b2c2ca1\n",
    "#### 4. https://towardsdatascience.com/methods-for-dealing-with-imbalanced-data-5b761be45a18\n",
    "#### 5.  https://medium.com/analytics-vidhya/confusion-matrix-accuracy-precision-recall-f1-score-ade299cf63cd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)\n",
      "              8231     cpu24 sparkclu kwjennin  R       1:29      2 b[05-06]\n",
      "              8195     cpu24 jupyterh kwjennin  R    2:44:31      1 b12\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "squeue -u kwjennin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Note on analysis of full dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data exploration of full dataset are provided in the notebook 'data_exploration_full_dataset.ipynb'.\n",
    "\n",
    "#### The original procedure we developed and started was done using the 'musical_instruments.json.gz' dataset. Unfortunately, we were unable to fit a model using the entire dataset of all 29 datasets merged together. We first attempted using the TALC cluster, however, the result was the model fitting never ending and then the kernel would be disrupted or access to TALC would timeout. This would disrupt the model fitting, so it could never be completed. \n",
    "\n",
    "#### We also tried creating a cluster on Google cloud services. The configuration of the cluster is outlined below. Because of the size of the files, it was not possible to upload all the data to Google storage. However, we were able to load all but one of the files, Books_5.json.gz, which is the largest file at approximately 7 GB. Like TALC, the Google cluster was able the perform all the data wrangling. But, even with the smaller amount of data, the cluster still could not fit the model. Unline TALC, the Google cluster would return an error message concerning a failed container from one of the nodes. Some investigation of the error appears to indicate that it concerns a memory or disk issue. So, it is possible that hte dataset is too large for the clusters, or there are too many tasks involved in fitting the model with this dataset that the cluster is being overwelmed. Another attempt was made on Google cloud services with a two node cluster with eight cores and 100 CB of disk each. This cluster still could not fit the model with the dataset.\n",
    "\n",
    "#### Due to the inability of the TALC and Google clusters to handle the entire dataset, we instead tried to fit the model to one of the individual dataset, the 'software.json.gz' dataset.\n",
    "\n",
    "#### The model fitting was run as a SLURM job on the cpu24 and cpu32-bigmem systems. The random forest classifier was able to be fit on a spark cluster on a cpu24 system over SLURM. The job ran for around 7 hours before it crashed. The accuracy of the model was found to be 65 %, however, the program failed and was cut off due to a lack of memory. This appears to confirm the issue encountered with the Google cluster.\n",
    "\n",
    "#### The notebooks for the analysis of the full dataset are 'pipeline_with_balanced_dataset.ipynb' and 'pipeline_with_unbalanced_dataset.ipynb'. The code in 'pipeline_with_balanced_dataset.ipynb' was run on cpu24 and cpu32-bigmem.\n",
    "\n",
    "#### All the jobs run on cpu32-bigmem did not complete. It appears that more system resources are necessary to perform this analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Google cloud cluster configuration\n",
    "\n",
    "gcloud beta dataproc clusters create cluster-1d88 --enable-component-gateway --bucket data608_bucket --region us-central1 --subnet default --zone us-central1-a --master-machine-type n1-standard-4 --master-boot-disk-size 32 --num-workers 5 --worker-machine-type n1-standard-2 --worker-boot-disk-size 32 --image-version 1.4-ubuntu18 --optional-components ANACONDA,JUPYTER --scopes 'https://www.googleapis.com/auth/cloud-platform' --project myds608test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

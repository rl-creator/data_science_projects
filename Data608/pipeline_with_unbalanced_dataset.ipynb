{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ryan Leeson, Keith Jennings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! scancel -u ryan.leeson -n sparkcluster "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sparkhpc.sparkjob:Submitted batch job 8796\n",
      "\n",
      "INFO:sparkhpc.sparkjob:Submitted cluster 2\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://b09:4043\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>spark://b05:7077</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=spark://b05:7077 appName=pyspark-shell>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os, atexit, sys, findspark, sparkhpc, pyspark\n",
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql import SQLContext\n",
    "\n",
    "findspark.init()\n",
    "\n",
    "# specify your partition (unless you're OK with default)\n",
    "os.environ['SBATCH_PARTITION']='cpu24'\n",
    "\n",
    "sj = sparkhpc.sparkjob.sparkjob(\n",
    "    ncores = 10,                       # total number or cores\n",
    "    cores_per_executor = 5,            # parallelism of two executor\n",
    "    memory_per_core = 10240,           # memory per core in MB \n",
    "    walltime = \"4:0\"                   # hh:mm format\n",
    ")\n",
    "\n",
    "sj.wait_to_start()\n",
    "sc = sj.start_spark()\n",
    "scq = SQLContext(sc)\n",
    "\n",
    "def exitHandler(sj,sc):\n",
    "    try:\n",
    "        print('Trapped Exit cleaning up Spark Context')\n",
    "        sc.stop()\n",
    "    except:\n",
    "        pass\n",
    "    try:\n",
    "        print('Trapped Exit cleaning up Spark Job')\n",
    "        sj.stop()\n",
    "    except:\n",
    "        pass\n",
    "atexit.register(exitHandler,sj,sc);\n",
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "\n",
    "from pyspark.ml.feature import StopWordsRemover\n",
    "\n",
    "from pyspark.ml.feature import Tokenizer, RegexTokenizer\n",
    "from pyspark.sql.functions import col, udf, concat\n",
    "from pyspark.sql import functions as f\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "from pyspark.sql.functions import input_file_name\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "from pyspark.sql.functions import split, lower\n",
    "\n",
    "from pyspark.ml.feature import NGram, FeatureHasher, CountVectorizer, RegexTokenizer\n",
    "\n",
    "from pyspark.ml.linalg import Vectors, VectorUDT\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "\n",
    "from itertools import chain\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "from pyspark import keyword_only\n",
    "from pyspark.ml import Transformer\n",
    "from pyspark.ml.param.shared import HasInputCol, HasOutputCol, Param, Params, TypeConverters\n",
    "from pyspark.ml.util import DefaultParamsReadable, DefaultParamsWritable  \n",
    "from pyspark.sql.functions import udf, create_map, lit\n",
    "from pyspark.sql.types import ArrayType, StringType\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "\n",
    "from pyspark.ml.tuning import TrainValidationSplit, ParamGridBuilder, CrossValidator\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.evaluation import RegressionEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = scq.read.parquet ('Files/appliance.parquet').select ('overall', 'reviewText')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+\n",
      "|overall|          reviewText|\n",
      "+-------+--------------------+\n",
      "|    5.0|I like this as a ...|\n",
      "|    5.0|           good item|\n",
      "|    5.0|Fit my new LG dry...|\n",
      "+-------+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show (3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 2277 entries in the dataset.\n"
     ]
    }
   ],
   "source": [
    "print (f'There are {df.count ()} entries in the dataset.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[overall: double, count: bigint, percent: double]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(df.groupBy ('overall')\n",
    " .count ()\n",
    " .withColumn ('percent', f.col ('count') / f.sum ('count').over (Window.partitionBy ()))\n",
    " .orderBy ('overall', ascending = False)\n",
    " #.show ()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After cleaning there are 145 entries remaining.\n"
     ]
    }
   ],
   "source": [
    "#   Remove properties with duplicate IDs\n",
    "\n",
    "df_cleaned = df.where (df.overall > 0).dropDuplicates ().na.drop (subset = ['reviewText'])\n",
    "\n",
    "print (f'After cleaning there are {df_cleaned.count ()} entries remaining.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_counts = (df_cleaned.groupBy ('overall')\n",
    " .count ()\n",
    " .withColumn ('percent', f.col ('count') / f.sum ('count').over (Window.partitionBy ()))\n",
    " .orderBy ('overall', ascending = False)\n",
    ")\n",
    "\n",
    "#display (df_counts.toPandas ())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1.0: 5.8, 4.0: 2.4166666666666665, 3.0: 4.142857142857143, 2.0: 4.142857142857143, 5.0: 0.2543859649122807}\n"
     ]
    }
   ],
   "source": [
    "#   get weights of classes\n",
    "\n",
    "#Begin Multinomial Logistic Regression with Weights\n",
    "\n",
    "#   To account for the unbalanced nature of the dataset in terms of the classes (5-star ratings),\n",
    "#   the weights are calculated to give more value to the classes with fewer entries and reduces \n",
    "#   the value of the classes with larger numbers of entries.\n",
    "\n",
    "#create column weights\n",
    "label_freq = df_cleaned.select(\"overall\").groupBy(\"overall\").count().collect()\n",
    "unique_label = [x[\"overall\"] for x in label_freq]\n",
    "total_label = sum([x[\"count\"] for x in label_freq])\n",
    "unique_label_count = len(label_freq)\n",
    "bin_count = [x[\"count\"] for x in label_freq]\n",
    "\n",
    "label_weights = {i: ii for i, ii in zip(unique_label, total_label / (unique_label_count * np.array(bin_count)))}\n",
    "print(label_weights)\n",
    "\n",
    "from itertools import chain\n",
    "\n",
    "mapping_expr = f.create_map([f.lit(x) for x in chain(*label_weights.items())])\n",
    "\n",
    "df_weighted = df_cleaned.withColumn(\"weight\", mapping_expr.getItem(f.col(\"overall\")))\n",
    "\n",
    "#source: https://danvatterott.com/blog/2019/11/18/balancing-model-weights-in-pyspark/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+------------------+\n",
      "|overall|          reviewText|            weight|\n",
      "+-------+--------------------+------------------+\n",
      "|    5.0|      Worked great!!|0.2543859649122807|\n",
      "|    5.0|works great and v...|0.2543859649122807|\n",
      "|    5.0|Worked great at h...|0.2543859649122807|\n",
      "|    5.0|works great instr...|0.2543859649122807|\n",
      "|    5.0|         received ok|0.2543859649122807|\n",
      "|    5.0|                  A+|0.2543859649122807|\n",
      "|    5.0|These start devic...|0.2543859649122807|\n",
      "|    4.0|Works great. Best...|2.4166666666666665|\n",
      "|    5.0|Worked great!! I ...|0.2543859649122807|\n",
      "|    2.0|             ok part| 4.142857142857143|\n",
      "|    3.0|       they ok parts| 4.142857142857143|\n",
      "|    5.0|This review is fo...|0.2543859649122807|\n",
      "|    5.0|Good value for el...|0.2543859649122807|\n",
      "|    5.0|This is the OEM f...|0.2543859649122807|\n",
      "|    5.0|This works great,...|0.2543859649122807|\n",
      "|    5.0|These filters wor...|0.2543859649122807|\n",
      "|    2.0|               cheap| 4.142857142857143|\n",
      "|    5.0|These are a great...|0.2543859649122807|\n",
      "|    1.0|Be careful, NewAi...|               5.8|\n",
      "|    4.0|I used this today...|2.4166666666666665|\n",
      "+-------+--------------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_weighted.show ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#   Custom transformers\n",
    "\n",
    "class PolarityTransformer (Transformer, HasInputCol, HasOutputCol, DefaultParamsReadable, DefaultParamsWritable):\n",
    "\n",
    "#   The polairtyTransformer uses the package 'TextBlob' to compute the pority of a string, how negative or how \n",
    "#   positive a string might be based on the words used. \n",
    "\n",
    "    @keyword_only\n",
    "    def __init__(self, inputCol=None, outputCol=None):\n",
    "        super (PolarityTransformer, self).__init__()\n",
    "        kwargs = self._input_kwargs\n",
    "        self.setParams(**kwargs)\n",
    "\n",
    "    @keyword_only\n",
    "    def setParams(self, inputCol=None, outputCol=None):\n",
    "        kwargs = self._input_kwargs\n",
    "        return self._set(**kwargs)\n",
    "\n",
    "    def _transform(self, dataset):\n",
    "        \n",
    "        def func (string):\n",
    "            return TextBlob (string).sentiment.polarity\n",
    "\n",
    "        t = StringType ()\n",
    "        out_col = self.getOutputCol ()\n",
    "        in_col = dataset[self.getInputCol ()]\n",
    "        return dataset.withColumn (out_col, udf (func, t)(in_col).cast ('double'))\n",
    "\n",
    "\n",
    "class WordSentimentTransformer (Transformer, HasInputCol, HasOutputCol, DefaultParamsReadable, DefaultParamsWritable):\n",
    "        \n",
    "#   Similar to PolarityTransformer, WordSentimentTransformer uses 'TextBlob' to evaluate the positivity or the \n",
    "#   negativity of a word. WordSentimentTransformer examines each word in a string, determines its polarity (positive,\n",
    "#   negative, or netural), and returns the counts of positive, negative, and neutral words as a vector.\n",
    "\n",
    "    @keyword_only\n",
    "    def __init__(self, inputCol=None, outputCol=None):\n",
    "        super (WordSentimentTransformer, self).__init__()\n",
    "        kwargs = self._input_kwargs\n",
    "        self.setParams(**kwargs)\n",
    "\n",
    "    @keyword_only\n",
    "    def setParams(self, inputCol=None, outputCol=None):\n",
    "        kwargs = self._input_kwargs\n",
    "        return self._set(**kwargs)\n",
    "\n",
    "    def _transform(self, dataset):\n",
    "        def WordsFreq (array):\n",
    "            \n",
    "            posWord = 0\n",
    "            negWord = 0\n",
    "            neutWord = 0\n",
    "\n",
    "            for word in array:\n",
    "                \n",
    "\n",
    "                if TextBlob (word).sentiment.polarity > 0:\n",
    "                    posWord += 1\n",
    "                elif TextBlob (word).sentiment.polarity < 0:\n",
    "                    negWord += 1\n",
    "                else:\n",
    "                    neutWord += 1\n",
    "\n",
    "            return posWord, negWord, neutWord\n",
    "\n",
    "        word_freq = udf (lambda a: Vectors.dense (WordsFreq (a)), VectorUDT ())\n",
    "\n",
    "        t = ArrayType (StringType ())\n",
    "        out_col = self.getOutputCol ()\n",
    "        in_col = dataset[self.getInputCol ()]\n",
    "        return dataset.withColumn (out_col, word_freq (in_col))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Classification\n",
    "\n",
    "We are attempting to train a random forest classifier to predict the star rating of a product besed on the review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RegexTokenizer (inputCol = \"reviewText\", outputCol = \"words\", pattern = \"\\\\W\", toLowercase = True)\n",
    "\n",
    "remover = StopWordsRemover (inputCol = tokenizer.getOutputCol (), outputCol = \"filtered\")\n",
    "\n",
    "wordsentiment = WordSentimentTransformer (inputCol = remover.getOutputCol (), outputCol = 'word_sentiment')\n",
    "\n",
    "polarity = PolarityTransformer (inputCol = 'reviewText', outputCol = 'polarity')\n",
    "\n",
    "\n",
    "assembler = VectorAssembler (inputCols = ['polarity', 'word_sentiment'], outputCol = 'features')\n",
    "\n",
    "\n",
    "#   Setting Random Forest Classifier\n",
    "\n",
    "randforest = RandomForestClassifier (labelCol = 'overall', featuresCol = assembler.getOutputCol (), numTrees = 10, maxMemoryInMB = 1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "(training, testing) = df_weighted.randomSplit ([0.8, 0.2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#   Setting Random Forest Classification mdoel\n",
    "randforest = RandomForestClassifier (labelCol = 'overall', featuresCol = assembler.getOutputCol (), numTrees = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#   Creating a pipeline for the randomforest classification\n",
    "#   During the pipeline the polarity of each review will be computed,\n",
    "#   the review will be tokenised,\n",
    "#   with the tokens, the stopwords will be removed,\n",
    "#   the polarity of each word in the array will be used to determine if the word is positive, negative, or\n",
    "#   neutral and a count for each category will be returned.\n",
    "#   The assembler will combine the polarity and word counts into a vector, 'features,' for the model fitting.\n",
    "#   The random forest classifier will using the 'overall' and 'features' columns from the training dataset to train\n",
    "#   the model.\n",
    "\n",
    "pipeline = Pipeline (stages = [polarity, tokenizer, remover, wordsentiment, assembler, randforest])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#   Fitting the model and saving the trained model\n",
    "\n",
    "rf_model = pipeline.fit (training)\n",
    "\n",
    "rf_model.save ('models/rf_model_unbalanced')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#   Model evaluation\n",
    "#   The model is evaluated for the prediction accuracy of the model and its F1-score.\n",
    "\n",
    "accuracy = evaluatorAcc.evaluate (rf_model.transform (testing))\n",
    "print (f'Test Error of logistic regression with cross valiation: {1.0 - accuracy}.')\n",
    "\n",
    "\n",
    "fscore = evaluatorf1.evaluate (rf_model.transform (testing))\n",
    "print (f'F1-score of logistic regression with cross validation: {fscore}.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because the PySpark RandomForestClassifier is not able to account for the unbalanced nature of the dataset, we will attempt a weighted logistic regression. The weights were calculated above and the values stored in the dataframe in the column 'weight.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "(training, testing) = df_weighted.randomSplit ([0.8, 0.2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#   Setting Logistic Regression model\n",
    "logreg = LogisticRegression (labelCol = 'overall', featuresCol = assembler.getOutputCol (), maxIter = 10, \n",
    "                             weightCol = 'weight', family = 'multinomial')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  The pipeline is like the one for random forest, but with the logistic regressor set above.\n",
    "\n",
    "pipeline2 = Pipeline (stages = [polarity, tokenizer, remover, wordsentiment, assembler, logreg])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#   Fitting the model and saving the trained model\n",
    "\n",
    "lr_weight_model = pipeline2.fit (training)\n",
    "\n",
    "lr_weight_model.save ('models/lr_weighted_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#   Model evaluation\n",
    "#   Evaluated using accuracy and F1-score\n",
    "\n",
    "evaluatorAcc = MulticlassClassificationEvaluator (predictionCol = 'prediction', labelCol = 'overall', metricName = \"accuracy\")\n",
    "\n",
    "accuracy = evaluatorAcc.evaluate (lr_weight_model.transform (testing))\n",
    "print (f'Test Error of logistic regression: {1.0 - accuracy}'.)\n",
    "\n",
    "\n",
    "evaluatorf1 = MulticlassClassificationEvaluator (predictionCol = 'prediction', labelCol = 'overall', metricName = 'f1')\n",
    "fscore = evaluatorf1.evaluate (lr_weight_model.transform (testing))\n",
    "print (f'F1-score of logistic regression: {fscore}.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic regression with hyperparameter tuning with cross validation.\n",
    "\n",
    "If possible we want to train a logistic regression model with hyperparameter tuning using cross validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "randforest = RandomForestClassifier (labelCol = 'overall', featuresCol = assembler.getOutputCol (), numTrees = 10, maxMemoryInMB = 1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "(training, testing) = df_weighted.randomSplit ([0.8, 0.2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg2 = LogisticRegression (labelCol = 'overall', featuresCol = assembler.getOutputCol (), maxIter = 10, \n",
    "                             weightCol = 'weight', family = 'multinomial')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cross validate\n",
    "grid = ParamGridBuilder().addGrid(logreg2.maxIter, [10, 20, 25]) \\\n",
    "                                .addGrid(logreg2.regParam, [0, 0.01, 0.05, 0.1, 0.5, 1]) \\\n",
    "                                .addGrid(logreg2.elasticNetParam, [0.0, 0.1, 0.5, 0.8, 1]) \\\n",
    "                                .build()\n",
    "\n",
    "logreg_cv = CrossValidator(estimator=logreg2, estimatorParamMaps=grid, \\\n",
    "                        evaluator=evaluatorf1, numFolds=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline3 = Pipeline (stages = [polarity, tokenizer, remover, wordsentiment, assembler, logreg_cv])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_cv_model = pipeline3.fit (training)\n",
    "\n",
    "lr_cv_model.save ('models/lr_weighted_model_crossval')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get performance metrics\n",
    "\n",
    "accuracy = evaluatorAcc.evaluate (lr_cv_model.bestModel.transform(testing))\n",
    "print (f'Test Error of logistic regression with cross valiation: {1.0 - accuracy}.')\n",
    "\n",
    "\n",
    "fscore = evaluatorf1.evaluate (lr_cv_model.bestModel.transform(testing))\n",
    "print (f'F1-score of logistic regression with cross validation: {fscore}.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Unfortunately, this code could not run on the cpu32-bigmem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Slurm job"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Script file to submit Slurm job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%file amazon_model_fitting_spark_unbalanced.script\n",
    "#!/bin/bash\n",
    "#SBATCH -J slurm-spark\n",
    "#SBATCH -t 1440 # runtime to request !!! in minutes !!!\n",
    "#SBATCH -o slurm-spark-%J.log # output extra o means overwrite\n",
    "#SBATCH -n 1 # requesting n tasks\n",
    "\n",
    "module load spark/jupyterhub\n",
    ". /global/software/jupyterhub-spark/anaconda3/etc/profile.d/conda.sh\n",
    "\n",
    "python amazon_model_fitting_spark_unbalanced.py > amazon_model_fitting_spark_unbalanced_results.txt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### .py file for fitting of unbalanced dataset\n",
    "\n",
    "This code was to run on the TALC cpu32-bigmem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%file amazon_model_fitting_spark_unbalanced.py\n",
    "#   Spark \n",
    "\n",
    "import os, atexit, sys, findspark, sparkhpc, pyspark\n",
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql import SQLContext\n",
    "\n",
    "findspark.init()\n",
    "\n",
    "# specify your partition (unless you're OK with default)\n",
    "os.environ['SBATCH_PARTITION']='cpu32-bigmem'\n",
    "\n",
    "sj = sparkhpc.sparkjob.sparkjob(\n",
    "    ncores = 24,                          # total number or cores\n",
    "    cores_per_executor = 24,              # parallelism of a single executor\n",
    "    memory_per_core = 1100 * 1024 // 24,  # memory per core in MB\n",
    "    walltime = \"14:0\"                      # hh:mm format\n",
    ")\n",
    "\n",
    "sj.wait_to_start()\n",
    "sc = sj.start_spark()\n",
    "scq = SQLContext(sc)\n",
    "\n",
    "def exitHandler(sj,sc):\n",
    "    try:\n",
    "        print('Trapped Exit cleaning up Spark Context')\n",
    "        sc.stop()\n",
    "    except:\n",
    "        pass\n",
    "    try:\n",
    "        print('Trapped Exit cleaning up Spark Job')\n",
    "        sj.stop()\n",
    "    except:\n",
    "        pass\n",
    "atexit.register(exitHandler,sj,sc);\n",
    "\n",
    "\n",
    "###########################################################################################################################\n",
    "\n",
    "\n",
    "from textblob import TextBlob\n",
    "\n",
    "from pyspark.ml.feature import StopWordsRemover\n",
    "\n",
    "from pyspark.ml.feature import Tokenizer, RegexTokenizer\n",
    "from pyspark.sql.functions import col, udf, concat\n",
    "from pyspark.sql import functions as f\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "from pyspark.sql.functions import input_file_name\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "from pyspark.sql.functions import split, lower\n",
    "\n",
    "from pyspark.ml.feature import NGram, FeatureHasher, CountVectorizer, RegexTokenizer\n",
    "\n",
    "from pyspark.ml.linalg import Vectors, VectorUDT\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "from itertools import chain\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "from pyspark import keyword_only\n",
    "from pyspark.ml import Transformer\n",
    "from pyspark.ml.param.shared import HasInputCol, HasOutputCol, Param, Params, TypeConverters\n",
    "from pyspark.ml.util import DefaultParamsReadable, DefaultParamsWritable  \n",
    "from pyspark.sql.functions import udf, create_map, lit\n",
    "from pyspark.sql.types import ArrayType, StringType\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "\n",
    "from pyspark.ml.tuning import TrainValidationSplit, ParamGridBuilder, CrossValidator\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "import time\n",
    "\n",
    "###########################################################################################################################\n",
    "\n",
    "start = time.time ()\n",
    "\n",
    "\n",
    "#   Bring in amazon_reviews.parquet\n",
    "df = scq.read.parquet ('Files/amazon_reviews.parquet').select ('overall', 'reviewText')\n",
    "\n",
    "#   Cleaning the dataset\n",
    "df_cleaned = df.where (df.overall > 0).dropDuplicates ().na.drop (subset = ['reviewText'])\n",
    "\n",
    "print (f'After cleaning there are {df_cleaned.count ()} entries remaining.')\n",
    "\n",
    "\n",
    "\n",
    "#   get weights of classes\n",
    "\n",
    "#Begin Multinomial Logistic Regression with Weights\n",
    "\n",
    "#create column weights\n",
    "label_freq = df_cleaned.select(\"overall\").groupBy(\"overall\").count().collect()\n",
    "unique_label = [x[\"overall\"] for x in label_freq]\n",
    "total_label = sum([x[\"count\"] for x in label_freq])\n",
    "unique_label_count = len(label_freq)\n",
    "bin_count = [x[\"count\"] for x in label_freq]\n",
    "\n",
    "label_weights = {i: ii for i, ii in zip(unique_label, total_label / (unique_label_count * np.array(bin_count)))}\n",
    "print(label_weights)\n",
    "\n",
    "mapping_expr = f.create_map([f.lit(x) for x in chain(*label_weights.items())])\n",
    "\n",
    "df_weighted = df_cleaned.withColumn(\"weight\", mapping_expr.getItem(f.col(\"overall\")))\n",
    "\n",
    "\n",
    "###########################################################################################################################\n",
    "\n",
    "#   Custom transformers\n",
    "\n",
    "class PolarityTransformer (Transformer, HasInputCol, HasOutputCol, DefaultParamsReadable, DefaultParamsWritable):\n",
    "    \n",
    "    @keyword_only\n",
    "    def __init__(self, inputCol=None, outputCol=None):\n",
    "        super (PolarityTransformer, self).__init__()\n",
    "        kwargs = self._input_kwargs\n",
    "        self.setParams(**kwargs)\n",
    "\n",
    "    @keyword_only\n",
    "    def setParams(self, inputCol=None, outputCol=None):\n",
    "        kwargs = self._input_kwargs\n",
    "        return self._set(**kwargs)\n",
    "\n",
    "    def _transform(self, dataset):\n",
    "        \n",
    "        def func (string):\n",
    "            return TextBlob (string).sentiment.polarity\n",
    "\n",
    "        t = StringType ()\n",
    "        out_col = self.getOutputCol ()\n",
    "        in_col = dataset[self.getInputCol ()]\n",
    "        return dataset.withColumn (out_col, udf (func, t)(in_col).cast ('double'))\n",
    "\n",
    "\n",
    "class WordSentimentTransformer (Transformer, HasInputCol, HasOutputCol, DefaultParamsReadable, DefaultParamsWritable):\n",
    "    \n",
    "    @keyword_only\n",
    "    def __init__(self, inputCol=None, outputCol=None):\n",
    "        super (WordSentimentTransformer, self).__init__()\n",
    "        kwargs = self._input_kwargs\n",
    "        self.setParams(**kwargs)\n",
    "\n",
    "    @keyword_only\n",
    "    def setParams(self, inputCol=None, outputCol=None):\n",
    "        kwargs = self._input_kwargs\n",
    "        return self._set(**kwargs)\n",
    "\n",
    "    def _transform(self, dataset):\n",
    "        def WordsFreq (array):\n",
    "            \n",
    "            posWord = 0\n",
    "            negWord = 0\n",
    "            neutWord = 0\n",
    "\n",
    "            for word in array:\n",
    "                \n",
    "\n",
    "                if TextBlob (word).sentiment.polarity > 0:\n",
    "                    posWord += 1\n",
    "                elif TextBlob (word).sentiment.polarity < 0:\n",
    "                    negWord += 1\n",
    "                else:\n",
    "                    neutWord += 1\n",
    "\n",
    "            return posWord, negWord, neutWord\n",
    "\n",
    "        word_freq = udf (lambda a: Vectors.dense (WordsFreq (a)), VectorUDT ())\n",
    "\n",
    "        t = ArrayType (StringType ())\n",
    "        out_col = self.getOutputCol ()\n",
    "        in_col = dataset[self.getInputCol ()]\n",
    "        return dataset.withColumn (out_col, word_freq (in_col))\n",
    "\n",
    "###########################################################################################################################\n",
    "\n",
    "#   Set transformers \n",
    "\n",
    "tokenizer = RegexTokenizer (inputCol = \"reviewText\", outputCol = \"words\", pattern = \"\\\\W\", toLowercase = True)\n",
    "\n",
    "remover = StopWordsRemover (inputCol = tokenizer.getOutputCol (), outputCol = \"filtered\")\n",
    "\n",
    "wordsentiment = WordSentimentTransformer (inputCol = remover.getOutputCol (), outputCol = 'word_sentiment')\n",
    "\n",
    "polarity = PolarityTransformer (inputCol = 'reviewText', outputCol = 'polarity')\n",
    "\n",
    "\n",
    "assembler = VectorAssembler (inputCols = ['polarity', 'word_sentiment'], outputCol = 'features')\n",
    "\n",
    "###########################################################################################################################\n",
    "\n",
    "#   Random Forest classifier\n",
    "\n",
    "randforest = RandomForestClassifier (labelCol = 'overall', featuresCol = assembler.getOutputCol (), numTrees = 10)\n",
    "\n",
    "\n",
    "(training, testing) = df_weighted.randomSplit ([0.8, 0.2])\n",
    "\n",
    "pipeline = Pipeline (stages = [polarity, tokenizer, remover, wordsentiment, assembler, randforest])\n",
    "\n",
    "rf_model = pipeline.fit (training)\n",
    "\n",
    "rf_model.save ('models/rf_model_unbalanced_amazon')\n",
    "\n",
    "\n",
    "accuracy = evaluatorAcc.evaluate (rf_model.transform (testing))\n",
    "print (f'Test Error of random forest classification with unbalanced dataset: {1.0 - accuracy}.')\n",
    "\n",
    "\n",
    "fscore = evaluatorf1.evaluate (rf_model.transform (testing))\n",
    "print (f'F1-score of random forest classification with unbalanced dataset: {fscore}.')\n",
    "\n",
    "###########################################################################################################################\n",
    "\n",
    "#   Logistic regression with weighted classes\n",
    "logreg = LogisticRegression (labelCol = 'overall', featuresCol = assembler.getOutputCol (), maxIter = 10, \n",
    "                             weightCol = 'weight', family = 'multinomial')\n",
    "\n",
    "(training, testing) = df_weighted.randomSplit ([0.8, 0.2])\n",
    "\n",
    "pipeline2 = Pipeline (stages = [polarity, tokenizer, remover, wordsentiment, assembler, logreg])\n",
    "\n",
    "lr_weight_model = pipeline2.fit (training)\n",
    "lr_weight_model.save ('models/lr_weighted_model_amazon')\n",
    "\n",
    "\n",
    "evaluatorAcc = MulticlassClassificationEvaluator (predictionCol = 'prediction', labelCol = 'overall', metricName = \"accuracy\")\n",
    "\n",
    "accuracy = evaluatorAcc.evaluate (lr_weight_model.transform (testing))\n",
    "print (f'Test Error of weighted logistic regression: {1.0 - accuracy}.')\n",
    "\n",
    "\n",
    "evaluatorf1 = MulticlassClassificationEvaluator (predictionCol = 'prediction', labelCol = 'overall', metricName = 'f1')\n",
    "fscore = evaluatorf1.evaluate (lr_weight_model.transform (testing))\n",
    "print (f'F1-score of weighted logistic regression: {fscore}.')\n",
    "\n",
    "\n",
    "\n",
    "print (f'Program took {time.time () - start} seconds.')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression with cross-validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Script file to submit Slurm job\n",
    "\n",
    "This job was not able to be submitted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%file amazon_model_fitting_spark_crossval.script\n",
    "#!/bin/bash\n",
    "#SBATCH -J slurm-spark\n",
    "#SBATCH -t 1440 # runtime to request !!! in minutes !!!\n",
    "#SBATCH -o slurm-spark-%J.log # output extra o means overwrite\n",
    "#SBATCH -n 1 # requesting n tasks\n",
    "\n",
    "module load spark/jupyterhub\n",
    ". /global/software/jupyterhub-spark/anaconda3/etc/profile.d/conda.sh\n",
    "\n",
    "python amazon_model_fitting_spark_crossval.py > amazon_model_fitting_spark_crossval_results.txt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### .py file for fitting of unbalanced dataset with cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%file \n",
    "\n",
    "#   Spark \n",
    "\n",
    "import os, atexit, sys, findspark, sparkhpc, pyspark\n",
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql import SQLContext\n",
    "\n",
    "findspark.init()\n",
    "\n",
    "# specify your partition (unless you're OK with default)\n",
    "os.environ['SBATCH_PARTITION']='cpu32-bigmem'\n",
    "\n",
    "sj = sparkhpc.sparkjob.sparkjob(\n",
    "    ncores = 24,                          # total number or cores\n",
    "    cores_per_executor = 24,              # parallelism of a single executor\n",
    "    memory_per_core = 1100 * 1024 // 24,  # memory per core in MB\n",
    "    walltime = \"13:0\"                      # hh:mm format\n",
    ")\n",
    "\n",
    "sj.wait_to_start()\n",
    "sc = sj.start_spark()\n",
    "scq = SQLContext(sc)\n",
    "\n",
    "def exitHandler(sj,sc):\n",
    "    try:\n",
    "        print('Trapped Exit cleaning up Spark Context')\n",
    "        sc.stop()\n",
    "    except:\n",
    "        pass\n",
    "    try:\n",
    "        print('Trapped Exit cleaning up Spark Job')\n",
    "        sj.stop()\n",
    "    except:\n",
    "        pass\n",
    "atexit.register(exitHandler,sj,sc);\n",
    "\n",
    "\n",
    "###########################################################################################################################\n",
    "\n",
    "\n",
    "from textblob import TextBlob\n",
    "\n",
    "from pyspark.ml.feature import StopWordsRemover\n",
    "\n",
    "from pyspark.ml.feature import Tokenizer, RegexTokenizer\n",
    "from pyspark.sql.functions import col, udf, concat\n",
    "from pyspark.sql import functions as f\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "from pyspark.sql.functions import input_file_name\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "from pyspark.sql.functions import split, lower\n",
    "\n",
    "from pyspark.ml.feature import NGram, FeatureHasher, CountVectorizer, RegexTokenizer\n",
    "\n",
    "from pyspark.ml.linalg import Vectors, VectorUDT\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "from itertools import chain\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "from pyspark import keyword_only\n",
    "from pyspark.ml import Transformer\n",
    "from pyspark.ml.param.shared import HasInputCol, HasOutputCol, Param, Params, TypeConverters\n",
    "from pyspark.ml.util import DefaultParamsReadable, DefaultParamsWritable  \n",
    "from pyspark.sql.functions import udf, create_map, lit\n",
    "from pyspark.sql.types import ArrayType, StringType\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "\n",
    "from pyspark.ml.tuning import TrainValidationSplit, ParamGridBuilder, CrossValidator\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "import time\n",
    "\n",
    "###########################################################################################################################\n",
    "\n",
    "start = time.time ()\n",
    "\n",
    "\n",
    "#   Bring in amazon_reviews.parquet\n",
    "df = scq.read.parquet ('Files/amazon_reviews.parquet').select ('overall', 'reviewText')\n",
    "\n",
    "#   Cleaning the dataset\n",
    "df_cleaned = df.where (df.overall > 0).dropDuplicates ().na.drop (subset = ['reviewText'])\n",
    "\n",
    "print (f'After cleaning there are {df_cleaned.count ()} entries remaining.')\n",
    "\n",
    "\n",
    "\n",
    "#   get weights of classes\n",
    "\n",
    "#Begin Multinomial Logistic Regression with Weights\n",
    "\n",
    "#create column weights\n",
    "label_freq = df_cleaned.select(\"overall\").groupBy(\"overall\").count().collect()\n",
    "unique_label = [x[\"overall\"] for x in label_freq]\n",
    "total_label = sum([x[\"count\"] for x in label_freq])\n",
    "unique_label_count = len(label_freq)\n",
    "bin_count = [x[\"count\"] for x in label_freq]\n",
    "\n",
    "label_weights = {i: ii for i, ii in zip(unique_label, total_label / (unique_label_count * np.array(bin_count)))}\n",
    "print(label_weights)\n",
    "\n",
    "from itertools import chain\n",
    "\n",
    "mapping_expr = f.create_map([f.lit(x) for x in chain(*label_weights.items())])\n",
    "\n",
    "df_weighted = df_cleaned.withColumn(\"weight\", mapping_expr.getItem(f.col(\"overall\")))\n",
    "\n",
    "\n",
    "###########################################################################################################################\n",
    "\n",
    "#   Custom transformers\n",
    "\n",
    "class PolarityTransformer (Transformer, HasInputCol, HasOutputCol, DefaultParamsReadable, DefaultParamsWritable):\n",
    "    \n",
    "    @keyword_only\n",
    "    def __init__(self, inputCol=None, outputCol=None):\n",
    "        super (PolarityTransformer, self).__init__()\n",
    "        kwargs = self._input_kwargs\n",
    "        self.setParams(**kwargs)\n",
    "\n",
    "    @keyword_only\n",
    "    def setParams(self, inputCol=None, outputCol=None):\n",
    "        kwargs = self._input_kwargs\n",
    "        return self._set(**kwargs)\n",
    "\n",
    "    def _transform(self, dataset):\n",
    "        \n",
    "        def func (string):\n",
    "            return TextBlob (string).sentiment.polarity\n",
    "\n",
    "        t = StringType ()\n",
    "        out_col = self.getOutputCol ()\n",
    "        in_col = dataset[self.getInputCol ()]\n",
    "        return dataset.withColumn (out_col, udf (func, t)(in_col).cast ('double'))\n",
    "\n",
    "\n",
    "class WordSentimentTransformer (Transformer, HasInputCol, HasOutputCol, DefaultParamsReadable, DefaultParamsWritable):\n",
    "    \n",
    "    @keyword_only\n",
    "    def __init__(self, inputCol=None, outputCol=None):\n",
    "        super (WordSentimentTransformer, self).__init__()\n",
    "        kwargs = self._input_kwargs\n",
    "        self.setParams(**kwargs)\n",
    "\n",
    "    @keyword_only\n",
    "    def setParams(self, inputCol=None, outputCol=None):\n",
    "        kwargs = self._input_kwargs\n",
    "        return self._set(**kwargs)\n",
    "\n",
    "    def _transform(self, dataset):\n",
    "        def WordsFreq (array):\n",
    "            \n",
    "            posWord = 0\n",
    "            negWord = 0\n",
    "            neutWord = 0\n",
    "\n",
    "            for word in array:\n",
    "                \n",
    "\n",
    "                if TextBlob (word).sentiment.polarity > 0:\n",
    "                    posWord += 1\n",
    "                elif TextBlob (word).sentiment.polarity < 0:\n",
    "                    negWord += 1\n",
    "                else:\n",
    "                    neutWord += 1\n",
    "\n",
    "            return posWord, negWord, neutWord\n",
    "\n",
    "        word_freq = udf (lambda a: Vectors.dense (WordsFreq (a)), VectorUDT ())\n",
    "\n",
    "        t = ArrayType (StringType ())\n",
    "        out_col = self.getOutputCol ()\n",
    "        in_col = dataset[self.getInputCol ()]\n",
    "        return dataset.withColumn (out_col, word_freq (in_col))\n",
    "\n",
    "###########################################################################################################################\n",
    "\n",
    "#   Set transformers \n",
    "\n",
    "tokenizer = RegexTokenizer (inputCol = \"reviewText\", outputCol = \"words\", pattern = \"\\\\W\", toLowercase = True)\n",
    "\n",
    "remover = StopWordsRemover (inputCol = tokenizer.getOutputCol (), outputCol = \"filtered\")\n",
    "\n",
    "wordsentiment = WordSentimentTransformer (inputCol = remover.getOutputCol (), outputCol = 'word_sentiment')\n",
    "\n",
    "polarity = PolarityTransformer (inputCol = 'reviewText', outputCol = 'polarity')\n",
    "\n",
    "\n",
    "assembler = VectorAssembler (inputCols = ['polarity', 'word_sentiment'], outputCol = 'features')\n",
    "\n",
    "\n",
    "\n",
    "evaluatorAcc = MulticlassClassificationEvaluator (predictionCol = 'prediction', labelCol = 'overall', metricName = \"accuracy\")\n",
    "evaluatorf1 = MulticlassClassificationEvaluator (predictionCol = 'prediction', labelCol = 'overall', metricName = 'f1')\n",
    "\n",
    "\n",
    "###########################################################################################################################\n",
    "\n",
    "#   Logistic Regression with cross-validation\n",
    "\n",
    "(training, testing) = df_weighted.randomSplit ([0.8, 0.2])\n",
    "\n",
    "logreg2 = LogisticRegression (labelCol = 'overall', featuresCol = assembler.getOutputCol (), maxIter = 10, \n",
    "                             weightCol = 'weight', family = 'multinomial')\n",
    "\n",
    "\n",
    "\n",
    "#cross validate\n",
    "grid = ParamGridBuilder().addGrid(logreg2.maxIter, [10, 20, 25]) \\\n",
    "                                .addGrid(logreg2.regParam, [0, 0.01, 0.05, 0.1, 0.5, 1]) \\\n",
    "                                .addGrid(logreg2.elasticNetParam, [0.0, 0.1, 0.5, 0.8, 1]) \\\n",
    "                                .build()\n",
    "\n",
    "logreg_cv = CrossValidator(estimator=logreg2, estimatorParamMaps=grid, \\\n",
    "                        evaluator=evaluatorf1, numFolds=3)\n",
    "\n",
    "pipeline = Pipeline (stages = [polarity, tokenizer, remover, wordsentiment, assembler, logreg_cv])\n",
    "\n",
    "\n",
    "lr_cv_model = pipeline2.fit (training)\n",
    "\n",
    "#lr_cv_model.save ('models/lr_weighted_model_crossval')\n",
    "\n",
    "\n",
    "#get performance metrics\n",
    "\n",
    "accuracy = evaluatorAcc.evaluate (lr_cv_model.bestModel.transform(testing))\n",
    "print (f'Test Error of logistic regression with cross valiation: {1.0 - accuracy}.')\n",
    "\n",
    "\n",
    "fscore = evaluatorf1.evaluate (lr_cv_model.bestModel.transform(testing))\n",
    "print (f'F1-score of logistic regression with cross validation: {fscore}.')\n",
    "\n",
    "\n",
    "\n",
    "print (f'Program took {time.time () - start} seconds.')\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
